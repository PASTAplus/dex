import contextlib
import functools
import logging
import pathlib
import re
import threading

import flask
import pandas as pd

import db
import filesystem

try:
    import cPickle as pickle
except ImportError:
    import pickle

import lxml.etree

import fasteners

threading_lock = threading.Lock()

log = logging.getLogger(__name__)


@contextlib.contextmanager
def lock(rid, key, obj_type):

    log.debug(f"Waiting to acquire lock: {rid}_{key}_{obj_type}")
    # with threading_lock:
    with fasteners.InterProcessLock(f"/tmp/dex_lock_{rid}_{key}_{obj_type}"):
        log.debug(f"Acquired lock: {rid}_{key}_{obj_type}")
        yield


def disk(key, obj_type, is_source_obj=False):
    """Disk cache decorator.

    The first arg to the wrapped function must be `rid`.

    Calls to this method are serialized using locks for performance. In the future, we
    might want to add hardware detection to check how many concurrent disk read/write
    operations the server can do before performance starts going down. On machines with
    only a single HDD or SSD, having a single global lock may be optimal.

    The general strategy is that we use nested calls that each may be filled from the
    cache. E.g., the function to get the head of a CSV file calls the function to get
    the full CSV and extracts the head from it. Serving a single page triggers a series
    of such functions. The goal is to cause the first function that acquires the lock to
    prepare the underlying objects while the other functions first wait and then can use
    the already prepared objects. Without locking, we'd end up loading the full CSV file
    into memory in many concurrent processes.

    Also important to note is that reading multiple files at the same time from the same
    HDD causes disk thrashing and extremely bad performance. The situation is much
    better on SSDs, but the bandwidth to the disk can be saturated, so, while there may
    not be a large disavantage to concurrent access as with HDDs, it's also not
    necessarly very beneficial.

    The locks also prevent attemts to read cached items while they're being written.

    Args:
        key:
        obj_type:
        is_source_obj (bool):
            False (default): The object is only cached if config.DISK_CACHE_ENABLED == True
            True: The object is always cached, regardless of the
            config.DISK_CACHE_ENABLED setting. This intended for use with the top level
            CSV file itself and any other objects that are not generated by Dex, and
            as such can be safely cached even on development systems.
    """

    def decorator(fn):
        @functools.wraps(fn)
        def wrapper(rid, *args, **kwargs):
            with lock(rid, key, obj_type):
                try:
                    if (
                        not flask.current_app.config["DISK_CACHE_ENABLED"]
                        and not is_source_obj
                    ):
                        raise KeyError
                    return read_from_cache(rid, key, obj_type)
                except KeyError:
                    obj = fn(rid, *args, **kwargs)
                    save_to_cache(rid, key, obj_type, obj)
                    return obj

        return wrapper

    return decorator


def is_cached(rid, key, obj_type):
    return get_cache_path(rid, key, obj_type).exists()


def read_from_cache(rid, key, obj_type):
    cache_path = get_cache_path(rid, key, obj_type)
    if not cache_path.exists():
        raise KeyError
    if obj_type == "df":
        return load_hdf(cache_path, key)
    elif obj_type == "eml":
        return load_eml(cache_path)
    elif obj_type in ("text", "csv", "html"):
        return load_utf8(cache_path)
    elif obj_type in ("lxml", "etree"):
        return load_lxml(cache_path)
    else:
        return load_pickle(cache_path)


def save_to_cache(rid, key, obj_type, obj):
    cache_path = get_cache_path(rid, key, obj_type, mkdir=True)
    if obj_type == "df":
        return save_hdf(cache_path, key, obj)
    elif obj_type == "eml":
        return save_eml(cache_path, obj)
    elif obj_type in ("text", "csv", "html"):
        return save_utf8(cache_path, obj)
    elif obj_type in ("lxml", "etree"):
        return save_lxml(cache_path, obj)
    else:
        return save_pickle(cache_path, obj)


#     dex.csv_cache.log.info(f'get_csv() start. rid="{rid}" key="{key_str}"')
#     start_ts = time.time()
#     dex.csv_cache.log.info(f'get_csv(): {time.time() - start_ts:.02f}s')


def save_hdf(cache_path, key, df):
    return df.to_hdf(cache_path, re.sub("[^a-z0-9]", key, "_"), mode="w")


def load_hdf(cache_path, key):
    try:
        return pd.read_hdf(cache_path, key)
    except AttributeError:
        # https://github.com/pandas-dev/pandas/issues/31199
        log.debug(
            f'Cannot read cached HDF. Discarding it. cache_path="{cache_path}"'
        )
        cache_path.unlink()
        raise KeyError
    except OSError:
        raise KeyError


def save_eml(cache_path, eml_str):
    return cache_path.write_text(eml_str, encoding="utf-8")


def load_eml(cache_path):
    return cache_path.read_text(encoding="utf-8")


def save_utf8(cache_path, csv_str):
    return cache_path.write_text(csv_str, encoding="utf-8")


def load_utf8(cache_path):
    return cache_path.read_text(encoding="utf-8")


def save_lxml(cache_path, obj):
    obj.write(cache_path.as_posix())


def load_lxml(cache_path):
    return lxml.etree.parse(cache_path.as_posix())


def save_pickle(cache_path, obj):
    with cache_path.open("wb") as f:
        return pickle.dump(obj, f)


def load_pickle(cache_path):
    with cache_path.open("rb") as f:
        return pickle.load(f)


def get_cache_path(rid, key, obj_type, mkdir=False):
    data_url = db.get_data_url(rid)
    cache_path = pathlib.Path(
        flask.current_app.config["CACHE_ROOT_DIR"],
        filesystem.get_safe_lossy_path_element(data_url),
        f"{key}.{obj_type}",
    )
    log.debug(f"cache_path: {rid} {key} {obj_type} -> {cache_path}")
    if mkdir:
        cache_path.parent.mkdir(parents=True, exist_ok=True)
    return cache_path
